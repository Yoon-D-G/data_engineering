{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Kafka Streams and API's </h1>\n",
    "\n",
    "\n",
    "\n",
    "## What is Kafka Streams and why is it important\n",
    "\n",
    "Kafka Streams is a __client library__  that provides an easy way to create real-time applications and microservices that process __constant flows of data__. Processing a continuous flow of data is known as streaming. It runs on top of the regular Kafka Consumer API, which is part of the main Kafka system.\n",
    " \n",
    " Kafka Streams interacts with input and output data and stores them in the Kafka cluster itself. Kafka Streams is not an entirely seperate application that runs independently.  Rather, it's a library that is called from within the context of already existing applications data engineers have developed (in Python, Java or Scala). \n",
    "\n",
    "Apache Kafka, Kafka Streams and Kafka Connect are all tools that form part of the Kafka ecosystem of event ingestion. These tools have some similarities, but there are also some key differences that set them apart:\n",
    "\n",
    "\n",
    "- __Apache Kafka__: is a publish-subscribe messaging system, used for both batch and real-time data ingestion. Applications publish a stream of events or messages to a topic on Kafka. The stream can be consumed independently by many consumers, and messages in the topic can even be replayed if needed. Kafka is massively scalable and fault-tolerant.\n",
    "\n",
    "- __Kafka Consumer_API__: in a nutshell, Kafka Consumer API allows applications to process messages from topics. It provides the basic components to interact with them, including the following capabilities:\n",
    "\n",
    "  - Separation of responsibility between consumers and producers\n",
    "  - Single processing\n",
    "  - Batch processing support\n",
    "  - Only stateless support. The client does not keep the previous state and evaluates each record in the stream individually\n",
    "  - No use of threading or parallelism\n",
    "  - It is possible to write to several Kafka clusters\n",
    "<p></p>\n",
    "\n",
    "- __Kafka Streams__: on the other hand, is an API for writing applications that transform and enrich real-time data in Apache Kafka, usually by publishing the transformed data to a new topic. The data processing itself happens within your application, not on a Kafka broker.\n",
    " \n",
    "- __Kafka Connect__: is an API for moving data into and out of Kafka. It standardises the integration of other applications into Kafka, by letting you write and share connectors for moving data to/from popular applications like databases.\n",
    "\n",
    "- __KSQL__: is a database that is purpose-built for Stream processing applications in Kafka.  It allows immediate response to events, creating materialized views over data streams, receive real-tiome push updates or pull the current state on-demand.  Essentially, this technology allows the use of SQL-like queries on top of data streams flowing through Kafka.\n",
    "\n",
    "Here is a visual representation of Kafka Streams:\n",
    "  <p align=\"center\">\n",
    "  <img src=\"./images/kafka-streams-1.png\" width=900>\n",
    "  </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Kafka Streams builds upon important stream-processing concepts such as:\n",
    "\n",
    "-  Properly distinguishing between event-time (the time at which an event is published to a topic) and processing-time (the time at which the data is processed)\n",
    "-  Windowing support (which allows grouping of data records that have the same key for stateful operations like joins)\n",
    "-  Simple yet efficient management and real-time querying of application states.\n",
    "\n",
    "Kafka Streams also has a low barrier to entry for new users - data engineers can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka's parallelism model.\n",
    "\n",
    "### Some of the important highlights of Kafka Streams include:\n",
    "\n",
    "- Designed as a simple and lightweight client library, which can be easily embedded in any Python, Java or Scala application and integrated with any existing packaging, deployment and operational tools that users have for their streaming applications. Kafka Streams doesn't require a seperate cluster to run, which is one of the main advantages of this technology.\n",
    "\n",
    "- Has no external dependencies on systems other than Apache Kafka itself as the internal messaging layer; notably, it uses Kafka's partitioning model to horizontally scale processing while maintaining strong ordering guarantees.\n",
    "\n",
    "- Supports fault-tolerant local states, which enables very fast and efficient stateful operations like windowed joins and aggregations, as well as stateless operations like filtering and mapping.\n",
    "\n",
    "- Supports exactly-once processing semantics to guarantee that each record will be processed once and only once even when there is a failure on either streams clients or Kafka brokers in the middle of processing. There is no micro-batching in Kafka Streams - each record is processed on its own.\n",
    "\n",
    "- Employs one-record-at-a-time processing to achieve millisecond processing latency, and supports event-time based windowing operations with out-of-order arrival of records.\n",
    "\n",
    "- Offers necessary stream processing primitives, along with a high-level Streams DSL and a low-level Processor API.\n",
    "\n",
    "- Kafka Streams is deployment agnostic, and can be deployed to containers, virtual machines, cloud infrastructure, on premise or on bare metal hardware.\n",
    "\n",
    "- The technology is fault-tolerant, elastic and easy to scale and can be used for small, medium or large use cases (minimum number of nodes is ideally 3)\n",
    "\n",
    "- Stream processing tasks are performed inside an existing application that the data engineers have developed.  Kafka Streams is simply a plugin library that runs within the context of our existing code. \n",
    "\n",
    "Here is a video explanation of Kafka Streams:\n",
    "- [__Introduction to Kafka Streams__](https://www.youtube.com/embed/Z3JKCLG3VP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream Topology\n",
    "\n",
    "- A __Stream__ is the most important abstraction provided by Kafka Streams; it represents an unbounded, continuously updating data set. A Stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a Key-Value pair.\n",
    "\n",
    "- A __Stream Processing Application__ is any program that makes use of the Kafka Streams library. It defines its computational logic through one or more processor topologies, where a processor topology is a graph of stream processors (nodes) that are connected by streams (edges).\n",
    "\n",
    "- A __Stream Processor__ is a node in the processor topology; it represents a processing step to transform data in streams by receiving one input record at a time from its upstream processors in the topology, applying its operation to it, and may subsequently produce one or more output records to its downstream processors.\n",
    "\n",
    "_There are two special processors in the topology:_\n",
    "\n",
    "- __Source Processor:__ A Source processor is a special type of stream processor that does not have any upstream processors. It produces an input stream to its topology from one or multiple Kafka topics by consuming records from these topics and forwarding them to its down-stream processors.\n",
    "\n",
    "- __Sink Processor:__ A Sink processor is a special type of stream processor that does not have down-stream processors. It sends any received records from its up-stream processors to a specified Kafka topic.\n",
    "\n",
    "Below is a high-level overview of the Stream Topology:\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/streams-architecture-topology.jpg\" width=400>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kafka Streams offers two ways to define the Stream processing topology:__ \n",
    "\n",
    "- Kafka __Streams DSL__ provides the most common data transformation operations such as _map_, _filter_, _join_ and _aggregations_ out of the box\n",
    "\n",
    "For a more detailed explanation of the Streams DSL, read the [Streams DSL guide here](https://kafka.apache.org/25/documentation/streams/developer-guide/dsl-api.html)\n",
    "\n",
    "- __Lower-level Processor API__ allows developers define and connect custom processors as well as to interact with state stores.\n",
    "\n",
    "For a more detailed explanation of the Processor API, read the [Processor API guide here](https://kafka.apache.org/25/documentation/streams/developer-guide/processor-api.html)\n",
    "\n",
    "\n",
    "A Processor topology is merely a logical abstraction for your stream processing code. At runtime, the logical topology is instantiated and replicated inside the application for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Kafka Streams Application\n",
    "\n",
    "Next, we'll roll up our sleeves and start work on our very own Kafka Streams application.\n",
    "\n",
    "We'll be using the WordCount application to become familiar with how Kafka Streams operates.\n",
    "\n",
    "The full step-by-step details are available on this link:\n",
    "\n",
    "- [Kafka Streams Official Demo App](https://kafka.apache.org/30/documentation/streams/quickstart)\n",
    "\n",
    "*_Note_: Ensure you're running the commands from the Kafka main folder that has both _bin_ and _config_ sub-folders available. Also make sure you went through all the steps in the previous lessons to correctly setup Kafka and Zookeeper on your local machine.\n",
    "\n",
    "1. Open a terminal and run the following command to start the zookeeper server and provide it with the properties file as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the ZooKeeper service\n",
    "bin/zookeeper-server-start.sh config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Open a 2nd terminal and run the following command to start the Kafka broker server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Kafka broker service\n",
    "bash bin/kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, prepare the data __input__ topic by running the following command using the standard mandatory arguments for running Kafka locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data input topic\n",
    "bash bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic streams-plaintext-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Required Arguments:__\n",
    "- `--create`:\n",
    "    -   Creates a new topic.  This is required ony the first time we are dealing with a new topic.\n",
    "- `--topic`:\n",
    "    -   The topic name to create, alter, describe or delete.\n",
    "- `--partitions`\n",
    "    -   The number of partitions for the topic being created or altered.\n",
    "- `--replication-factor`:\n",
    "    -   The replication factor for each partition in the topic being created.  This is mandatory if there is no default setup in the cluster itself.\n",
    "- `--bootstrap-server`:\n",
    "    -   The Kafka server to connect to.  Localhost:9092 is to be used in case of a local stand-alone environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Prepare the __output__ topic.  We can also assign a compaction mechanism which can be used to control data retention and make it per-record rather than time-based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data output topic\n",
    "bash bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic streams-wordcount-output --config cleanup.policy=compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the mandatory arguments mentioned in the command above this one, the additional configuration is:\n",
    "- `--config cleanup.policy`:\n",
    "    -   This argument designates the retention policy to use on old data segments. The default policy, \"delete\", will discard old segments when their retention time or size limit has been reached. The \"compact\" setting will enable log compaction on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Check that the topics have been created successfully by using the `--describe` feature of the `kafka-topics` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists the available topics\n",
    "bash bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look something like this:\n",
    "\n",
    "- `Topic:streams-wordcount-output\tPartitionCount:1\tReplicationFactor:1\tConfigs:cleanup.policy=compact,segment.bytes=1073741824`\n",
    "\t`Topic: streams-wordcount-output\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0`\n",
    "- `Topic:streams-input              PartitionCount:1\tReplicationFactor:1\tConfigs:segment.bytes=1073741824`\n",
    "\t`Topic: streams-plaintext-input\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Next, we need to start the Wordcount application. To do this, run the following command in a new terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Wordcount application\n",
    "bash bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters:__\n",
    "- `kafka-run-class.sh`:\n",
    "    -   Calls the main Kafka class bash script used to run an application\n",
    "- `org.apache.kafka.streams.examples.wordcount.WordCountDemo`\n",
    "    -   The URL of the application file Kafka should run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demo application will read from the input topic streams-input, perform the computations of the WordCount algorithm on each of the read messages, and continuously write its current results to the output topic streams-wordcount-output. \n",
    "\n",
    "Hence there won't be any `STDOUT` output in the terminal, except log entries as the results are written back into in Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Process some data in real-time\n",
    "\n",
    "Now let's create some message with the console producer and feed them into the input topic `streams-plaintext-input`.  You can achieve this by entering a single line of text and then hiting the `<RETURN>` key to move to the next line of input data. \n",
    "\n",
    "This will send a new message to the input topic, where the message key is _null_ and the message _value_ is the string encoded text line that you just entered (in practice, input data for applications will typically be streaming continuously into Kafka, rather than being manually entered as we do in this introductory tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the producer and create input data by typing text in the console terminal\n",
    "bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic streams-plaintext-input\n",
    "`Jeff Materrson New York United States Senior Data Engineer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Inspect the output of the WordCount application by reading from its _output_ topic using the console consumer. To do this, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Consumer using the output Topic created earlier\n",
    "bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic streams-wordcount-output --from-beginning --formatter kafka.tools.DefaultMessageFormatter --property print.key=true --property print.value=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Required arguments:__\n",
    "- `topic`:\n",
    "    -   Topic name from which to consume the data records.\n",
    "- `bootstrap-server`:\n",
    "    -   The Kafka server to connect to.\n",
    "\n",
    "__Important optional arguments:__\n",
    "\n",
    "- `from-beginning`\n",
    "    -   Tells the consumer that if it doesn't already have an established offset to consume from, start with the earliest message present in the log.\n",
    "- `formatter`:\n",
    "    -    The name of the class to use for formatting Kafka records being printed.\n",
    "- `property`:\n",
    "    -   The consumer configuration properties can be used to specify certain key configurations\n",
    "        -   `print.key=true`: configures the console consumer to print the keys of the messages it consumes.  Default is _false_\n",
    "        -   `print.value=true`: When we are not concerned with printing the values of the records being consumed, you can set this property which configures the console consumer to either print or not print values of records being consumed. The default value is true.\n",
    "        -   `key.deserializer`: the deserializer class to use for the data record key\n",
    "        -   `value.deserializer`: the deserializer class to use for the data record value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look something like:\n",
    "- `Jeff\t        1`\n",
    "- `Matterson\t    1`\n",
    "- `New\t        1`\n",
    "- `York\t        1`\n",
    "- `United   \t    1`\n",
    "- `States        1`\n",
    "- `Senior        1`\n",
    "- `Data          1`\n",
    "- `Engineer      1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first column is the Kafka message key in `java.lang.String` format and represents a word that is being counted, and the second column is the message value in `java.lang.Longformat`, representing the word's latest count.\n",
    "\n",
    "9. Now let's continue writing one more message with the console producer into the input topic streams-plaintext-input. \n",
    "\n",
    "Enter the below text and hit the `<RETURN>` key. Your terminal should look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more text in the Producer\n",
    "bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic streams-plaintext-input\n",
    "`John Doe York United Kingdom Data Engineer`\n",
    "`Francois Marc Paris France Data Scientist`\n",
    "`Kumar Gupta Mumbai India Data Engineer`\n",
    "`Jeff McDonald Boston United States Data Scientist`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Consumer terminal, the output should be updated dynamically and the count should be updated.\n",
    "- `Jeff\t        2`\n",
    "- `Matterson\t    1`\n",
    "- `New\t        1`\n",
    "- `York\t        2`\n",
    "- `United   \t    2`\n",
    "- `States        2`\n",
    "- `Senior        1`\n",
    "- `Data          4`\n",
    "- `Engineer      3`\n",
    "- `John          1`\n",
    "- `Doe           1`\n",
    "- `United Kingdowm  1`\n",
    "- `Francois      1`\n",
    "- `Marc          1`\n",
    "- `Paris         1`\n",
    "- `France        1`\n",
    "- `Scientist     2`\n",
    "- `Kumar         1`\n",
    "- `Gupta         1`   \n",
    "- `Mumbai        1`\n",
    "- `India         1`\n",
    "- `McDonald      1`\n",
    "- `Boston        1`\n",
    "\n",
    "\n",
    "\n",
    "The reason count increased is because Kafka Streams is operating on the data while it's in-motion.  The output of the Wordcount application is actually a continuous stream of updates, where each record is an updated count of a single word.  If the data producer keeps sending data, the consumer will keep updating in real-time as the data arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Shut down the application\n",
    "\n",
    "Finally, you can now stop all console operations and terminate the process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "- Kafka Streams is different than Kafka Core itself, but that it runs on top of its infrastructure.\n",
    "- A Stream is an unbounded, continuously updating dataset.\n",
    "- Kafka Streams supports a variety of operations on data in-motion including SQL-like operations.\n",
    "- Kafka Streams is a client library available for use by Data Engineers and can be called from within their code.\n",
    "- Kafka Streams doesn't need a seperate cluster to run.\n",
    "- Kafka Streams Sources send the data and Sinks recieve the data while streams are the data records themselves.\n",
    "- We need both the Zookeeper and Kafka Server operating in order to produce and consume messages."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
