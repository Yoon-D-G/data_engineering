{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spark\r\n",
    "<p align=center><a href=https://spark.apache.org/><img src=images/spark-logo-trademark.png width=300></a></p>\r\n",
    "\r\n",
    "> <font size=+1>Spark is a unified engine for large-scale distributed data processing on computer clusters</font>\r\n",
    "\r\n",
    "It was originally written in [__Scala__](https://www.scala-lang.org/) programming language, and its open source project is available on [GitHub](https://github.com/apache/spark).\r\n",
    "\r\n",
    "Spark provides in-memory storage for intermediate computations, and it is designed considering four key points:\r\n",
    "\r\n",
    "1. Speed: Spark's framework takes advantages of the current hardware improvements. It uses DAGs and query optimizers that allow it to run multiple tasks in parallel. \r\n",
    "\r\n",
    "2. Ease of use: Spark offers a simple programming model so that high-level data structures (DataFrames for example) are handled using familiar languages.\r\n",
    "\r\n",
    "3. Modularity: Spark supports different programming languages (Python, Java, Scala, and R), and thus, it has unified libraries that run under a single engine (Tungsten). That means that you can write a single Spark application that can do all the tasks you need.\r\n",
    "\r\n",
    "4. Extensibility: Spark doesn't focus on storage, and as such, the developers made it compatible with many myriad sources. You can see the connections in the following image:\r\n",
    "\r\n",
    "<p align=center><img src=images/Spark_Connections.png width=400></p>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the main uses of Spark is parallelizing computations, hiding all the complexity of distributions. That way data engineers can focus on high-level operations, such as ETL. \r\n",
    "\r\n",
    "However, Spark integrates many tools, such as Spark MLlib which offers a set of ML algorithms to build model pipelines. \r\n",
    "\r\n",
    "Thus, Spark is not just a Data Engineering tool. Some popular use cases of Spark are:\r\n",
    "\r\n",
    "- Processing large datasets distributed across a cluster\r\n",
    "- Performing queries to explore and visualize datasets\r\n",
    "- Implementing end-to-end data pipelines from myriad sources\r\n",
    "- Analyzing Graph datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we will explore the theory behind Spark, so the operations you perform in next lessons are more sensible!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "## Supported language frontends\r\n",
    "\r\n",
    "Official APIs are provided for different languages:\r\n",
    "- [PySpark](https://spark.apache.org/docs/latest/api/python/) - as the name suggests Python frontend for Spark\r\n",
    "- [Java API](https://sparkjava.com/) - as Scala is based off JVM and Java language with high interoperability before both languages\r\n",
    "- [SparkR](https://spark.apache.org/docs/latest/sparkr.html) - [R langauge](https://www.r-project.org/) front-end for statistical oriented code\r\n",
    "\r\n",
    "> __We will use PySpark in order to interact with Spark engine__\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "## High level libraries\r\n",
    "\r\n",
    "High level libraries are provided on top top of `Spark`, namely:\r\n",
    "- [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) - Query language for data processing\r\n",
    "- [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) - Machine Learning on Spark computing engine\r\n",
    "- [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) - graph related operations\r\n",
    "- [Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) - streaming related operations\r\n",
    "\r\n",
    "> __In this notebook we will focus on core Spark functionalities__\r\n",
    "\r\n",
    "> Other functionalities can be used on the same engine, __please refer to documentation if you need specific part in your workflow__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cluster mode overview\r\n",
    "\r\n",
    "Now, you know that Spark is a distributed data processing engine, and its components are working on a cluster. Before we dive in, let's see what the engine consists of in more detail and how can we choose one.\r\n",
    "\r\n",
    "Spark applications usually run on __clusters__ and consists of:\r\n",
    "\r\n",
    "- Spark Driver, which in turn contains the SparkSession\r\n",
    "- Cluster Manager\r\n",
    "- Spark Executor\r\n",
    "\r\n",
    "From the image below, you can observe that a Spark application consists on a driver program that orchestrates parallel operations on a cluster. The Spark Application contains information about the session, which is used by the driver to access the executors (inside the nodes) and the manager.\r\n",
    "\r\n",
    "<p align=center><img src=images/pyspark-driver-executor.png width=500></p>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's observe these components more in detail"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spark Driver\r\n",
    "\r\n",
    "The Spark Driver intaintiate a SparkSession, and it is also responsible for:\r\n",
    "\r\n",
    "- Communicating with the __Cluster Manager__\r\n",
    "- Requesting resources from the __Cluster Manager__ to allocate those to the Executors\r\n",
    "- Orchestrating and scheduling the Spark operations\r\n",
    "- After allocating the resources, it communicates with the executors to 'tell' them the schedule\r\n",
    "    - __Sends code to the executors__, one of:\r\n",
    "        - Python files (in case of PySpark)\r\n",
    "        - JAR files for Scala/Java code\r\n",
    "    - __Sends tasks to the executors__, which are __single unit of work send to a single executor__\r\n",
    "\r\n",
    "> The code is sent via `spark-submit` scripts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SparkSession\r\n",
    "\r\n",
    "SparkSession is a unified conduit for Spark operations and data. Through this conduit, Spark can communicate with its surroundings:\r\n",
    "\r\n",
    "- It can create runtime parameters for the executors (JVM - Java Virtual Machine)\r\n",
    "- Define Dataframe and Datasets\r\n",
    "- Read from data sources\r\n",
    "- Send SQL queries\r\n",
    "\r\n",
    "SparkSessions is one of the core components of a Spark application, so the high-level API is available in a variety of programming languages.\r\n",
    "\r\n",
    "<font size=-2>In Spark 1.x, SparkContext was used. In newer versions, you can still see SparkContext, and SparkSessions included backward compatibility to include code containing SparkContext<font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cluster Manager\r\n",
    "\r\n",
    "> <font size=+1>Cluster manager is a program handling resources to our application(s)</font>\r\n",
    "\r\n",
    "Cluster manager is responsible for:\r\n",
    "- Handling requests from a driver for resources\r\n",
    "\r\n",
    "There are a few available options, most important of which are:\r\n",
    "- Local - run everything on a single machine (__non distributed!__)\r\n",
    "- [Standalone](https://spark.apache.org/docs/latest/spark-standalone.html) - PySpark \"default\" cluster manager\r\n",
    "- [Apache Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html) - Apache Spark \"modern\" approach, useful for __more generic workloads__\r\n",
    "- [Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html) - Apache Spark \"older\" approach, specific for Hadoop oriented operations (e.g. map-reduce)\r\n",
    "- [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html) - container first auto-scalable workloads\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Local\r\n",
    "\r\n",
    "> Use it when you are developing and testing your app\r\n",
    "\r\n",
    "As it is the simplest one, we can verify everything works correctly using a single machine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standalone\r\n",
    "\r\n",
    "> Small clusters WORKING ONLY WITH PYSPARK APPLICATION\r\n",
    "\r\n",
    "This one, while it doesn't require additional software has the falling drawbacks:\r\n",
    "- We cannot run other workload on it (e.g. monitoring)\r\n",
    "- PySpark first\r\n",
    "- __Runs main and child processes of PySpark on each node__ hence it has an additional overhead"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mesos\r\n",
    "\r\n",
    "> __Larger/production clusters with GENERAL capabilities__\r\n",
    "\r\n",
    "- Better for new projects\r\n",
    "- More generic than YARN\r\n",
    "- Good option for non-containerized "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## YARN (Hadoop 2.0)\r\n",
    "\r\n",
    "> __Larger/production clusters with GENERAL capabilities BETTER AT RUNNING HADOOP SPECIFIC OPERATIONS__\r\n",
    "\r\n",
    "Other than that quite similar to Mesos\r\n",
    "\r\n",
    "<p align=center><img src=images/spark-standalone-hadoop.png width=400></p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kubernetes\r\n",
    "\r\n",
    "> __Workloads which can autoscale (create more/less instances based on workload) and containerized__\r\n",
    "\r\n",
    "Using Kubernetes (also named `k8s`) one has a lot of benefits and becomes a go-to for the following reasons:\r\n",
    "- We can containerize most of the applications\r\n",
    "- Because of that our deployment is streamlined and less error-prone (different OS different behaviour)\r\n",
    "- __Autoscaling__ - create more node workers if needed\r\n",
    "- __Available as service for many clouds__ ([Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks/), [Google Cloud Kubernetes Engine](https://www.google.com/search?client=firefox-b-d&q=Google+Cloud+Kubernetes+engine) or [Microsoft's Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/))\r\n",
    "\r\n",
    "This approach scales well across:\r\n",
    "- different regions if needed\r\n",
    "- different cloud providers if needed\r\n",
    "- for smaller teams (for which handling Kubernetes cluster is too costly) via out-of-the-box cloud solutions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nomad\r\n",
    "\r\n",
    "> __Workloads mixing containerized and non-containerized workloads across large amount of clusters__\r\n",
    "\r\n",
    "Similiar to `k8s` but:\r\n",
    "- No autoscaling out of the box (needs additional software for that)\r\n",
    "- Smaller community support\r\n",
    "- __Easier to use than Kubernetes__\r\n",
    "- __Less popular than Kubernetes__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Executor\r\n",
    "\r\n",
    "> __Processes which run computations and store data__\r\n",
    "\r\n",
    "Data can be stored in a few different ways which we will later talk about (see `Data Locality` below).\r\n",
    "\r\n",
    "Things to note:\r\n",
    "- __Each application has a single executor on the node__\r\n",
    "- __There might be multiple executors on a single node__\r\n",
    "- Due to above applications are isolated (each is run in a separate JVM machine)\r\n",
    "- __DATA CANNOT BE EASILY SHARED BETWEEN SPARK APPLICATIONS__ (we need to save the data in some widely available storage like Kubernetes volumes for other apps to use)\r\n",
    "\r\n",
    "## Useful things to note\r\n",
    "\r\n",
    "> __See [glossary](https://spark.apache.org/docs/latest/cluster-overview.html) for a quick reminder of all of the concepts__\r\n",
    "\r\n",
    "- __Job is a set of parallel tasks__ distributed across the cluster, for example `collect` across nodes\r\n",
    "- __Driver should be close to workers__ (or most of them) as it orchestrates the whole workload (best when in the same local network if possible)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HDFS (Hadoop Distributed FileSystem)\r\n",
    "\r\n",
    "<p><a href='https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#Introduction'><img src=images/hdfs.png width=200></a></p>\r\n",
    "\r\n",
    "As different cluster managers can handle data differently, hence data could be (theoretically):\r\n",
    "- shared across cluster\r\n",
    "- shared across parts of the cluster\r\n",
    "- kept local for each node\r\n",
    "- kept local and pulled/moved around\r\n",
    "\r\n",
    "> __This would affect computation speeds tremendously!__ \r\n",
    "\r\n",
    "Apache has an answer to that: __HDFS__\r\n",
    "<p align=center><img src=images/hdfsarchitecture.png width=500></a></p>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Hadoop Distributed File System (HDFS) provides high throughput access to application data and is suitable for applications that have large data sets. This file system will allow us to prevent the aforementioned problems throughout an operation named MapReduce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MapReduce\r\n",
    "\r\n",
    "> __Processing layer over distributed filesystem designed for processing large volumes of data in parallel by dividing work into a set of independent tasks__\r\n",
    "\r\n",
    "The idea works as follows:\r\n",
    "- User submits __job__ (usually large amount of work) which consists of:\r\n",
    "    - Execution of __Mapper__\r\n",
    "    - Execution of __Reducer__\r\n",
    "- __Job__ is splitted into tasks (done in parallel by worker nodes)\r\n",
    "- These are sent to child processes on cluster\r\n",
    "- __Individual Mapper and Reducer executions are done on each node__\r\n",
    "- Each task returns an output which is latter aggregated to give final result\r\n",
    "\r\n",
    "> __MapReduce operates on lists!__\r\n",
    "\r\n",
    "This means:\r\n",
    "- Input to our functions are lists\r\n",
    "- Procedures outputs lists\r\n",
    "- __Functional programming approach__ (data is unmutable)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task Attempt\r\n",
    "\r\n",
    "> Each node can attempt to perform a task (Task In Progress a.k.a. TIP status) __but may fail due to various reasons__\r\n",
    "\r\n",
    "If a node fails:\r\n",
    "- Hadoop reschedules the task to other node\r\n",
    "- It can be done multiple times (__up to `4` by default__)\r\n",
    "- After that program fails\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "## High level flow\r\n",
    "\r\n",
    "<p align=center><img src=images/map_reduce_counting.jpg width=600></p>\r\n",
    "\r\n",
    "Let's see how we obtain results step by step by extending the diagram above:\r\n",
    "1. Our input data (usually saved in HDFS), in this case text\r\n",
    "2. `InputFormat` defines:\r\n",
    "    - __How to split data__\r\n",
    "    - __How to read them__\r\n",
    "    - __Creates `InputSplit`s__\r\n",
    "3. __`InputSplit`s represent data processed by each `Mapper`__:\r\n",
    "    - One `map` task for each split\r\n",
    "    - `InputSplit` is divided into separate records\r\n",
    "    - And these records are processed by `map` operation\r\n",
    "4. __`RecordReader`__ communicates with `InputSplit` to:\r\n",
    "    - Transform the split into readable format for mapper (`(key, value)` pairs)\r\n",
    "5. __`Mapper`__ - processes `(key, value)` pair from `RecordeReader` and:\r\n",
    "    - Generates new `(key, value)` pair\r\n",
    "    - __Does it by our specified logic__ (in this case counting word occurences)\r\n",
    "    - Outputs values to disk creating __temporary results__ (__THESE ARE NOT SAVED TO HDFS!__)\r\n",
    "6. __`Combiner`__ (a.k.a. `mini-reducer`) - takes temporary values and:\r\n",
    "    - Combines them into larger batches\r\n",
    "    - This is done in order to minimize data transfers over the network\r\n",
    "7. __`Partitioner`__ (__USED ONLY FOR MULTIPLE `Reducer`s__):\r\n",
    "    - Takes output from `combiner`\r\n",
    "    - __`key` is used to make a single partition__ (in our case specific word)\r\n",
    "    - __Records having the same `(key, value)`s are called a partition__\r\n",
    "    - __GUARANTEES APPROXIMATELY THE SAME LOAD FOR EACH `Reducer`__\r\n",
    "8. __Shuffling and Sorting__ - data set via network to Reducer notes:\r\n",
    "    - __Each `Reducer` might get multiple partitions__\r\n",
    "    - Each partition is sorted so they are __a consecutive block of data__\r\n",
    "9. __`Reducer`__ - takes combined values from previous step and:\r\n",
    "    - Runs __user defined reduction operation__ on each temporary `(key, value)` pair \r\n",
    "    - In our case it counts how many of the same records are there\r\n",
    "    - __Stores output on HDFS__ via `RecordWriter`\r\n",
    "    - We can modify it (e.g. in Java) via specifying custom `OutputFormat` ([documentation](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/OutputFormat.html))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Map Reduce FAQ\r\n",
    "\r\n",
    "There might be a few misconceptions, so let's clear them out:\r\n",
    "\r\n",
    "> Why shuffle happens?\r\n",
    "\r\n",
    "It happens as chunks of data are moved across the network. They might:\r\n",
    "- come at different times\r\n",
    "- __from any node in the HDFS__\r\n",
    "\r\n",
    "Hence they are unorganized on disk __and that's why we have to sort them afterwards__\r\n",
    "\r\n",
    "> How many mappings are run on one node?\r\n",
    "\r\n",
    "Usually around `100` parallel tasks __per node__ are run. For lighter tasks, up to `300` is reasonable\r\n",
    "\r\n",
    "> Why sorting twice?\r\n",
    "\r\n",
    "__This is done only for multiple `Reducers`__ in order to:\r\n",
    "- Make the network congestion smaller (because single partition will land on a single `Reducer`)\r\n",
    "- There might be multiple \"same\" partitions (based on `(key, value)`) from different mappers\r\n",
    "- Multiple partitions might be processed by one node\r\n",
    "\r\n",
    "Let's look at the last example:\r\n",
    "1. There are `3` `A` and `3` `B` partitions in total in HDFS network\r\n",
    "2. Each partition lies on different mapper node\r\n",
    "3. Intermediate results are send to `Reducers`\r\n",
    "\r\n",
    "We might obtain the following (__already sorted by mappers!__) data scheme: `ABABABAB`. This means we have to sort them once again.\r\n",
    "\r\n",
    "> Can `Reducer` run when some of the `mappers` did not finish?\r\n",
    "\r\n",
    "__No__ as it might mean \"reduce\" operation would need to be recalculated. This is done only after \"aggregating\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spark vs Hadoop\r\n",
    "\r\n",
    "Hadoop itself was mentioned a few times, what is it and what's the difference between it and Spark?\r\n",
    "\r\n",
    "> __Hadoop consists of HDFS, MapReduce computational layer and YARN (Hadoop cluster manager)__\r\n",
    "\r\n",
    "Written in `Java` and released in `2006`, provides multiple front-end languages to interact as well. When compared to `Spark`:\r\n",
    "\r\n",
    "- It is a batch-processing large-scale data-efficient processing framework\r\n",
    "- __DOES NOT PROVIDE REAL-TIME CAPABILITIES FOR CALCULATIONS__ because:\r\n",
    "    - Writing to disk all the time is too slow\r\n",
    "    - Solved by `Spark`\r\n",
    "- __`Spark` DOES IT'S COMPUTATIONS IN-MEMORY WHEREVER POSSIBLE__:\r\n",
    "    - Not writing to disk intermediate results from nodes (__or at least does not for a part of data which fits in RAM__)\r\n",
    "    - Due to above `Spark` is about `100` times faster\r\n",
    "- __Spark can use multiple cluster managers__\r\n",
    "- __Spark is more of a \"high-level\" tool__ which uses various concepts from `Hadoop` and applies abstraciton layer over it\r\n",
    "- __It does not provide specific functionalities__ like Spark (e.g. `MLLib`)\r\n",
    "- __It is a part of Apache Spark__ (e.g. using `HDFS` and `YARN` as cluster manager)\r\n",
    "\r\n",
    "Essentially we have all of the pieces of Hadoop in place (at least in theory)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Locality\r\n",
    "\r\n",
    "> __Data Locality is the process of moving COMPUTATIONS closer to DATA__ (so they are run locally a.k.a. \"per-node\")\r\n",
    "\r\n",
    "In general, if `data` and `operations` reside close to each other the whole computation will be fast.\r\n",
    "In other cases, these might be slower, hence __computation has to be moved towards data__.\r\n",
    "\r\n",
    "There are a few possibilities when it comes to data locality in Spark (__ordered by best to worst__):\r\n",
    "1. `PROCESS_LOCAL` - __code is in the same `JVM` as data__ \r\n",
    "2. `NODE_LOCAL` - __data on the same node__, for example:\r\n",
    "    - HDFS on the same node\r\n",
    "    - Another executor on the same node\r\n",
    "    - __Data has to travel between processes__\r\n",
    "3. `NO_PREF` - data has no preference where it is located because:\r\n",
    "    - It does not matter for computation\r\n",
    "    - __Example:__ shared volumes in `k8s`\r\n",
    "4. `RACK_LOCAL` - data on the same rack of servers, __data has to be send through a single switch in the network__\r\n",
    "5. `ANY` - data is elsewhere on the network, __not in the same rack__\r\n",
    "\r\n",
    "__When `Spark` does scheduling for the computations it does it w.r.t. data locality__ which means:\r\n",
    "- `Spark` checks whether best node to process data is available\r\n",
    "- If not `Spark` waits for the busy `CPU` with best data locality to finish it's computation __but only for a short while__ \r\n",
    "- If it does not finish in a predefined `timeout` __spark moves data to next free `CPU`__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "> <font size=+1>One can control data locality via `spark.locality` setting we will later see</font>\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "> <font size=+1>YOU SHOULD INCREASE TIMEOUT IF YOU SEE POOR DATA LOCALITY WITH DEFAULT SETTINGS!</font>\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "> <font size=+1>Timeouts should be traced to how long your jobs run on the cluster</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assessment\r\n",
    "\r\n",
    "- Check out [Apache Myriad Project](https://incubator.apache.org/projects/myriad.html) - what might be it's benefits for cluster management in PySpark?\r\n",
    "- What is secondary `NameNode` and what is it's purpose in Hadoop's FileSystem?\r\n",
    "- Check out how to work with Hadoop's FileSystem via command line using this series of tutorials ([1](https://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/), [2](https://data-flair.training/blogs/hadoop-hdfs-commands/) and [3](https://data-flair.training/blogs/hdfs-hadoop-commands/))\r\n",
    "- Why `Partitioner` __IS NOT__ used in Hadoop MapReduce processing layer with single `Reducer`?\r\n",
    "\r\n",
    "\r\n",
    "## Non-assessment\r\n",
    "\r\n",
    "- What is SIMR approach included in the second graphic in this notebook? Check out [this article](https://databricks.com/blog/2014/01/21/spark-and-hadoop.html)\r\n",
    "- What is RAID and how does Erasure Coding work in Hadoop? Check [this tutorial](https://data-flair.training/blogs/hadoop-hdfs-erasure-coding/)\r\n",
    "- What are `BackupNode`s and `CheckpointNode`s in HDFS?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "conda-env-.conda-AiCore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}