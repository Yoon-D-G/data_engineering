{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Missing Data\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn how to identify and visualise missing data\n",
    " - For numerical, categorical and time-series data\n",
    "- The different types of missing values\n",
    "- The appropiate techniques to deal with missing data\n",
    " - Dropping data\n",
    " - Using the mean (but please don't use the mean)\n",
    " - Imputation\n",
    "- Imputing time series and categorical data\n",
    " \n",
    "In the Data Cleaning lesson, we observed that entries of our data were missing. This **missing data** problem occurs with almost every dataset. Data can go missing because of a wide range of reasons. Perhaps the most common reason is a 'faulty' data acquisition process (e.g. defective sensors for measuring temperature data, incomplete patient information etc), however some other reasons could include accidental data deletion or human error.\n",
    "\n",
    "Regardless of how data went missing (well.. not actually regardless - we'll see later on how knowing how data went missing may help our analysis), our job as data scientists is to treat our data based on safe and valid assumptions.\n",
    "\n",
    "Generally speaking, dealing with and treating missing data follows this pipeline:\n",
    "- Identify and convert missing values to null values\n",
    "- Analyse how much data is missing, and the type of missing-ness it is\n",
    "- Either delete the rows with missing values, or impute the missing values\n",
    "\n",
    "In this lesson, I will show examples on the two following datasets: <br>\n",
    "https://archive.ics.uci.edu/ml/datasets/Automobile <br>\n",
    "https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "## Identifying Missing Values\n",
    "One arbitray dataset could present missing values with a variety of differents 'placeholders' for missing values (even over the span of one column!) Examples of common missing values include: `NA`, `-`, `UNKNOWN` etc. The data dictionary/documentation is the first thing you should look at as it may describe how missing values are stored/formatted in the dataset. We should also perform checks by hand - one way to identify missing values is to return the unique values for a column, and sort them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# From the data documentatiobn:\r\n",
    "names = [\"symboling\", \"normalized-losses\", \"make\", \"fuel-type\", \"aspiration\", \"num-of-doors\", \"body-style\", \"drive-wheels\", \"engine-location\", \"wheel-base\", \"length\", \"width\", \"height\", \"curb-weight\", \"engine-type\", \"num-of-cylinders\", \"engine-size\", \"fuel-system\", \"bore\", \"stroke\", \"compression-ratio\", \"horsepower\", \"peak-rpm\", \"city-mpg\", \"highway-mpg\", \"price\"]\r\n",
    "auto_df = pd.read_csv(\"Data/imports-85.data\", header=None, names=names)\r\n",
    "auto_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel-type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>body-style</th>\n",
       "      <th>drive-wheels</th>\n",
       "      <th>engine-location</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>...</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>fuel-system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>94.5</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154</td>\n",
       "      <td>5000</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-1</td>\n",
       "      <td>95</td>\n",
       "      <td>volvo</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>109.1</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>9.5</td>\n",
       "      <td>114</td>\n",
       "      <td>5400</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>16845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-1</td>\n",
       "      <td>95</td>\n",
       "      <td>volvo</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>109.1</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>8.7</td>\n",
       "      <td>160</td>\n",
       "      <td>5300</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>19045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-1</td>\n",
       "      <td>95</td>\n",
       "      <td>volvo</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>109.1</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.87</td>\n",
       "      <td>8.8</td>\n",
       "      <td>134</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>21485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>-1</td>\n",
       "      <td>95</td>\n",
       "      <td>volvo</td>\n",
       "      <td>diesel</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>109.1</td>\n",
       "      <td>...</td>\n",
       "      <td>145</td>\n",
       "      <td>idi</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.40</td>\n",
       "      <td>23.0</td>\n",
       "      <td>106</td>\n",
       "      <td>4800</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>22470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-1</td>\n",
       "      <td>95</td>\n",
       "      <td>volvo</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>109.1</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>9.5</td>\n",
       "      <td>114</td>\n",
       "      <td>5400</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>22625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symboling normalized-losses         make fuel-type aspiration  \\\n",
       "0            3                 ?  alfa-romero       gas        std   \n",
       "1            3                 ?  alfa-romero       gas        std   \n",
       "2            1                 ?  alfa-romero       gas        std   \n",
       "3            2               164         audi       gas        std   \n",
       "4            2               164         audi       gas        std   \n",
       "..         ...               ...          ...       ...        ...   \n",
       "200         -1                95        volvo       gas        std   \n",
       "201         -1                95        volvo       gas      turbo   \n",
       "202         -1                95        volvo       gas        std   \n",
       "203         -1                95        volvo    diesel      turbo   \n",
       "204         -1                95        volvo       gas      turbo   \n",
       "\n",
       "    num-of-doors   body-style drive-wheels engine-location  wheel-base  ...  \\\n",
       "0            two  convertible          rwd           front        88.6  ...   \n",
       "1            two  convertible          rwd           front        88.6  ...   \n",
       "2            two    hatchback          rwd           front        94.5  ...   \n",
       "3           four        sedan          fwd           front        99.8  ...   \n",
       "4           four        sedan          4wd           front        99.4  ...   \n",
       "..           ...          ...          ...             ...         ...  ...   \n",
       "200         four        sedan          rwd           front       109.1  ...   \n",
       "201         four        sedan          rwd           front       109.1  ...   \n",
       "202         four        sedan          rwd           front       109.1  ...   \n",
       "203         four        sedan          rwd           front       109.1  ...   \n",
       "204         four        sedan          rwd           front       109.1  ...   \n",
       "\n",
       "     engine-size  fuel-system  bore  stroke compression-ratio horsepower  \\\n",
       "0            130         mpfi  3.47    2.68               9.0        111   \n",
       "1            130         mpfi  3.47    2.68               9.0        111   \n",
       "2            152         mpfi  2.68    3.47               9.0        154   \n",
       "3            109         mpfi  3.19    3.40              10.0        102   \n",
       "4            136         mpfi  3.19    3.40               8.0        115   \n",
       "..           ...          ...   ...     ...               ...        ...   \n",
       "200          141         mpfi  3.78    3.15               9.5        114   \n",
       "201          141         mpfi  3.78    3.15               8.7        160   \n",
       "202          173         mpfi  3.58    2.87               8.8        134   \n",
       "203          145          idi  3.01    3.40              23.0        106   \n",
       "204          141         mpfi  3.78    3.15               9.5        114   \n",
       "\n",
       "     peak-rpm city-mpg highway-mpg  price  \n",
       "0        5000       21          27  13495  \n",
       "1        5000       21          27  16500  \n",
       "2        5000       19          26  16500  \n",
       "3        5500       24          30  13950  \n",
       "4        5500       18          22  17450  \n",
       "..        ...      ...         ...    ...  \n",
       "200      5400       23          28  16845  \n",
       "201      5300       19          25  19045  \n",
       "202      5500       18          23  21485  \n",
       "203      4800       26          27  22470  \n",
       "204      5400       19          25  22625  \n",
       "\n",
       "[205 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's look at missing values for stroke\r\n",
    "np.sort(auto_df[\"stroke\"].unique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great - we see a \"?\" which is an indication of a missing value. Let's check a couple of other columns to ensure this value is consistent throughout the dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Unique values in price:\", np.sort(auto_df[\"price\"].unique()))\r\n",
    "print(\"Unique values in normalized losses:\", np.sort(auto_df[\"normalized-losses\"].unique()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question marks in both! Ok, this indicates that this missing value is probably consistent throughout the dataframe (and that more than one type of missing value doesn't exist). From the result of the `.info()` method above, note that although we would expect `stroke` to be of type float, Pandas is indicating to us that it is of type object. This happens because for Pandas, the native missing value is either `np.nan` or `pd.NA`. Now that know what the missing value is - let's reload the data, this time passing the missing value to the `na_values` argument in the `.read_csv()` constructor. The `na_values` flag looks at the string we've provided as the argument, and replaces that string with a `nan` value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "auto_df = pd.read_csv(\"Data/imports-85.data\", header=None, names=names, na_values=\"?\")\r\n",
    "auto_df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205 entries, 0 to 204\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   symboling          205 non-null    int64  \n",
      " 1   normalized-losses  164 non-null    float64\n",
      " 2   make               205 non-null    object \n",
      " 3   fuel-type          205 non-null    object \n",
      " 4   aspiration         205 non-null    object \n",
      " 5   num-of-doors       203 non-null    object \n",
      " 6   body-style         205 non-null    object \n",
      " 7   drive-wheels       205 non-null    object \n",
      " 8   engine-location    205 non-null    object \n",
      " 9   wheel-base         205 non-null    float64\n",
      " 10  length             205 non-null    float64\n",
      " 11  width              205 non-null    float64\n",
      " 12  height             205 non-null    float64\n",
      " 13  curb-weight        205 non-null    int64  \n",
      " 14  engine-type        205 non-null    object \n",
      " 15  num-of-cylinders   205 non-null    object \n",
      " 16  engine-size        205 non-null    int64  \n",
      " 17  fuel-system        205 non-null    object \n",
      " 18  bore               201 non-null    float64\n",
      " 19  stroke             201 non-null    float64\n",
      " 20  compression-ratio  205 non-null    float64\n",
      " 21  horsepower         203 non-null    float64\n",
      " 22  peak-rpm           203 non-null    float64\n",
      " 23  city-mpg           205 non-null    int64  \n",
      " 24  highway-mpg        205 non-null    int64  \n",
      " 25  price              201 non-null    float64\n",
      "dtypes: float64(11), int64(5), object(10)\n",
      "memory usage: 41.8+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above, we see that `stroke`, and some other columns have been converted to correct datatypes. \n",
    "\n",
    "NOTE: Historically, Pandas did not support `nan` types for integer numbers - which is why we see some of the above numbers that we would expect to be ints as floats. Recently they have introduced the `Int64` (capital I) type which does have support for a first party null type: `pd.NA`. You can read more here: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "\n",
    "We'll now be working with a diabetes dataset known as Pima. This was a study about diabetes on a Native American group of people known as Pima people. Let's load in the dataset and see if we can identify null values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pima_df = pd.read_csv(\"Data/datasets_228_482_diabetes.csv\")\r\n",
    "print(pima_df.info())\r\n",
    "pima_df"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This particular dataset is interesting because sometimes missing values aren't as obvious as having an explicit value. If we return the describe method, we might be able to find some questionable summary statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pima_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Is there any *row* which particularly stands out to you as questionable?\n",
    "\n",
    "\n",
    "The `min` row is particularly interesting because many of the values there are 0. Do you know any alive person who has a `BloodPressure` of 0? Or `Glucose`, `SkinThickness`, `Insulin` or `BMI` for that matter. It is obviously possible to have 0 `Pregnancies` though, so that value of 0 could be considered correct. For the questionable columns we identified, let's check how many 0 values are present."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "questionable_columns = [\"BloodPressure\", \"Glucose\", \"SkinThickness\", \"Insulin\", \"BMI\"]\r\n",
    "zero_counts = {col: 0 for col in questionable_columns}\r\n",
    "for col in questionable_columns:\r\n",
    "    zero_counts[col] = pima_df[col][pima_df[col] == 0].count()\r\n",
    "    \r\n",
    "print(zero_counts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we had performed visualisation on our data, identifying this would have been easier. Let's plot a histogram (which will be formally introduced later) and use `BloodPressure` as an example. Note that Plotly Express handles a LOT of things under the hood - and 'binning' float values is one of these things. We need to keep such things in mind when using high level libraries as otherwise we may make incorrect assumptions about our data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "fig = px.histogram(pima_df, \"BloodPressure\")\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so we can't pass \"0\" as an argument to `na_values` `.read_csv()` like we did previously because some of our 0 values are legitimate. Instead, we'll have to 'manually' replace these values with `np.nan`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Replace 0 with np.nan for the questionable columns\r\n",
    "\r\n",
    "\r\n",
    "## describe the dataframe\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The amount of missingness\n",
    "It can be valuable to know how much of our data is actually missing - in terms of absolute values or percentages. Doing so is a relatively straightforward process which I will demonstrate on our `auto_df`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_df_null = auto_df.isnull() # .isna() is the same as .isnull()\r\n",
    "auto_df_null"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We've now obtained a dataframe with True/False values - True indicating where there is a missing/null value, and False otherwise. To see the absolute amount of missing values, we can `.sum()` the dataframe. To obtain the percentages, we can simply do a `.mean() * 100`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_df_null.sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Using one line only, work out the total percentage of missing values from the pima dataframe\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualising Missing Data\n",
    "There is a very useful package called `missingno` which allows us to easily visualise our data and identify rows where our data is missing. Doing this allows you to graphically visualise which rows have missing data, and can help us to determine whether data has gone missing because of a random error or because of something a bit more systematic.\n",
    "\n",
    "For example, in the auto_df plot, we see that where "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import missingno as msno\r\n",
    "\r\n",
    "msno.matrix(auto_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# msno.bar(pima_df)\r\n",
    "msno.matrix(pima_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## When (and how) to delete data\r\n",
    "\r\n",
    "There are two types of deletion we can consider:\r\n",
    "\r\n",
    "1. Pairwise Deletion\r\n",
    "2. Listwise Deletion\r\n",
    "\r\n",
    "**Pairwise deletion** is when missing values are skipped during the calculation of some statistic. This is almost a non-factor with Pandas because when we compute a statistic, missing values aren't considered:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Mean of Glucose using native mean method:\", pima_df[\"Glucose\"].mean())\r\n",
    "print(\"Mean of Glucose via manual calculation  :\", pima_df[\"Glucose\"].sum() / pima_df[\"Glucose\"].count())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Listwise deletion** is when we drop the whole row of data because of a missing value. This is completely valid to do but should only be done when the amount of rows you are dropping is insignificant when compared to the rest of the dataset. From our counts of nullity that we did previously, and working under the knowledge that `Glucose` and `BMI` are MCAR, we are safe to drop the rows where these missing values are present. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Drop the rows in Glucose and BMI for which there are missing values\r\n",
    "\r\n",
    "\r\n",
    "## Plot a missingno matrix of the dataframe\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputing using averages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Imputation** is the act of predicting the missing data, and can be applied to any of the missing data classifications we are working with. In this section, we will look at imputing data using central tendancy measures. We will show why this may not be a great idea, and then introduce ML imputing techniques. Recall the three types of average: Mean, Median and Mode.\n",
    "\n",
    "The values of these types of imputations are trivial to calculate and we could easily do it ourselves if we wanted to. However, for the sake of introducing you to the API, we will be using skleanr's `SimpleImputer`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pima_cols = pima_df.columns\r\n",
    "pima_cols"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\r\n",
    "\r\n",
    "mode_imputer = SimpleImputer(strategy=\"most_frequent\")\r\n",
    "pima_mode_arr = mode_imputer.fit_transform(pima_df)\r\n",
    "pima_mode_df = pd.DataFrame(data=pima_mode_arr, columns=pima_cols)\r\n",
    "pima_mode_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Impute and assign dataframes for strategies of mean and median\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### So why is this bad?\n",
    "\n",
    "Central tendancy imputations should be avoided because they reduce the variance of the data, leading to a higher bias in the data. Perhaps more intuitively, these types of imputations will not consider any of the other variable relationships in your data. We can easily see this with visualisations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nulls = (pima_df[\"SkinThickness\"].isnull() + pima_df[\"BMI\"].isnull()).astype(\"int\")\r\n",
    "px.scatter(pima_mean_df, x=\"SkinThickness\", y=\"BMI\", title=\"SkinThickness vs BMI (Mean Imputation)\",\r\n",
    "           color=nulls)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising the different imputations through subplots\r\n",
    "from plotly.subplots import make_subplots\r\n",
    "\r\n",
    "fig_mean = px.scatter(pima_mean_df, x=\"SkinThickness\", y=\"BMI\", color=nulls)\r\n",
    "\r\n",
    "fig_median = px.scatter(pima_median_df, x=\"SkinThickness\", y=\"BMI\", color=nulls)\r\n",
    "\r\n",
    "fig_mode = px.scatter(pima_mode_df, x=\"SkinThickness\", y=\"BMI\", color=nulls)\r\n",
    "\r\n",
    "fig = make_subplots(rows=1, cols=3, shared_xaxes=False, subplot_titles=(\"Mean Imputation\",\"Median Imputation\", \"Mode Imputation\"))\r\n",
    "fig.add_trace(fig_mean['data'][0], row=1, col=1)\r\n",
    "fig.add_trace(fig_median['data'][0], row=1, col=2)\r\n",
    "fig.add_trace(fig_mode['data'][0], row=1, col=3)\r\n",
    "fig.update_layout(title=\"SkinThickness vs BMI (Imputations)\")\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ML based imputation techniques\n",
    "\n",
    "The alternative to using central tendencies or constant values for imputations is to use ML based imputation methods. We will cover three types algorithms here which have popular use: Nearest neighbours imputation, tree based imputation and regression based imputation. These methods work by building models for a feature based on the other features of the data.\n",
    "\n",
    "As you've already covered the underlying algorithms of KNNs, Ensemble Trees and Regression, we won't recap them here - just show how to apply them to our dataset.\n",
    "\n",
    "We'll start with KNN imputation. Here, the algorithm selects the K nearest/most similar datapoints to a datapoint with a missing value. The missing value is then either populated with an average from the K neighbours, or a weighted average. This argument can be specified by `weights` flag in sklearn's [`KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer) class. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.impute import KNNImputer\r\n",
    "\r\n",
    "# default arguments are:\r\n",
    "# n_neighbors = 5\r\n",
    "# weights = \"uniform\"\r\n",
    "knn_impute = KNNImputer(n_neighbors=3, weights=\"distance\")\r\n",
    "pima_knn_arr = knn_impute.fit_transform(pima_df)\r\n",
    "pima_knn_df = pd.DataFrame(data=pima_knn_arr, columns=pima_cols)\r\n",
    "\r\n",
    "fig_knn = px.scatter(pima_knn_df, x=\"SkinThickness\", y=\"BMI\", title=\"SkinThickness vs BMI (KNN Imputation)\",\r\n",
    "           color=nulls)\r\n",
    "fig_knn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When researching imputation techniques, one common method you'll come across is something known as [**MICE**](https://www.jstatsoft.org/article/view/v045i03) - Multivariate Imputation by Chained Equations. This algorithm performs multiple regressions of a random sample of data, and uses the average of these multiple regressions to impute the missing value. With the sklearn API, the appropiate method to use is the [`IterativeImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) class, passing `sample_posterior=True` ([source](https://scikit-learn.org/stable/modules/impute.html))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# IterativeImputer is an experimental feature in sklearn, so we need to enable it prior to using it\r\n",
    "from sklearn.experimental import enable_iterative_imputer\r\n",
    "from sklearn.impute import IterativeImputer\r\n",
    "\r\n",
    "regression_impute = IterativeImputer(sample_posterior=True)\r\n",
    "pima_regression_arr = regression_impute.fit_transform(pima_df)\r\n",
    "pima_regression_df = pd.DataFrame(data=pima_regression_arr, columns=pima_cols)\r\n",
    "\r\n",
    "fig_br = px.scatter(pima_regression_df, x=\"SkinThickness\", y=\"BMI\", title=\"SkinThickness vs BMI (Regression Imputation)\",\r\n",
    "           color=nulls)\r\n",
    "fig_br"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `IterativeImputer` class is highly flexible and allows us to use any estimator object to perform our imputation (instead of just regression). The default estimator that it uses isn't actually vanilla linear regression - it's something known as Bayesian Ridge regression. We won't cover the details here as the important thing to know is that it is just a linear regression variant. If you are curious about a fuller understanding, more details can be found at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge.\n",
    "\n",
    "Another, more recent, trend in imputation is using tree based ensembles. Here we will use the `RandomForestRegressor` estimator to impute the missing values:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "\r\n",
    "## Impute the missing values using RandomForestRegressor and IterativeImputer\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = make_subplots(rows=1, cols=3, shared_xaxes=False, subplot_titles=(\"KNN Imputation\", \"BR Regression Imputation\", \"RF Regression Imputation\"))\r\n",
    "fig.add_trace(fig_knn['data'][0], row=1, col=1)\r\n",
    "fig.add_trace(fig_br['data'][0], row=1, col=2)\r\n",
    "fig.add_trace(fig_rf['data'][0], row=1, col=3)\r\n",
    "fig.update_layout(title=\"SkinThickness vs BMI (Regression Imputations)\")\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Series Imputation\n",
    "\n",
    "Time series data referes to data that has been collected over time, with each datapoint being indexed by a sequential datetime. The most typical example is perhaps stock data. Here, we will look at the Beijing PM2.5 Data available from: https://www.kaggle.com/joshuapaulbarnard/beijing-air-quality-pm25-from-2010-to-2017?select=Beijing+PM2_5+from+2010+to+2017.csv. In the `.read_csv()` method, we'll tell Pandas that we want to index on the Date variable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_df_org = pd.read_csv(\"Data/Beijing PM2_5 from 2010 to 2017.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_df = pm_df_org[:400000]\r\n",
    "pm_df = pm_df.loc[~pm_df.index.duplicated(keep='last')]\r\n",
    "pm_df = pm_df.sort_index(axis=0)\r\n",
    "pm_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Check the percentage of nulls in each column\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Renaming columns so they're easier to refer to\r\n",
    "pm_cols = [\"city\", \"country\", \"season\", \"pm25\", \"dew_point\", \"temperature\", \"humidity\", \"pressure\", \"wind_direction\", \"wind_speed\", \"precipitation_hourly\", \"preciptiation_cum\"]\r\n",
    "pm_df.columns = pm_cols\r\n",
    "pm_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fill missing values\n",
    "\n",
    "We will start with the [`.fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) method. The `.fillna()` method does what the name implies - it fills values where there is a NaN. This method has two strategies we can adopt:\n",
    "- `ffill`\n",
    "- `bfill`\n",
    "\n",
    "Which stand for \"forward fill\" and \"backward fill\" respectively. If we have a Series with some missing values in it, `ffill` will replace the NaNs with the last non-NaN value we've come across in the Series - until the next non-NaN value is reached. Backward fill reverses this and fills in the non-NaN values with the *next* non-NaN value we would observe. Seeing this in action will clarify this description:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_df[\"wind_speed\"][31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_ffill_df = pm_df.fillna(method=\"ffill\")\r\n",
    "pm_ffill_df[\"wind_speed\"][31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_bfill_df = pm_df.fillna(method=\"bfill\")\r\n",
    "pm_bfill_df[\"wind_speed\"][31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpolation\n",
    "\n",
    "Ok - so this works to fill NA values, but it's not really ideal. (Note that the fillna method has applications outside of time series data). The next level up is the [`.interpolation()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html) function. Interpolation is art of finding a transition from one point to the other. `.interpolation()` has many useful techniques to interpolate datapoints, but we will focus on the following 3. Refer to the documentation for a more comprehensive list, and the recommended place for when to use the method:\n",
    "- Linear\n",
    "- Quadratic\n",
    "- Nearest\n",
    "\n",
    "#### Linear\n",
    "Linear interpolation simply fills values with equidistant points from the first non-NaN value to the next. Visually:\n",
    "![](../images/linear_interpolation.png)\n",
    "\n",
    "It would be trivial to calculate what this 'equidistant' value actually is - we take the first non-NaN value and the following non-NaN value, subtract the two together, and divide it by the number of NaNs between the two points. With the `wind_speed` data we were looking at, this would be $(2.4 - 1.3)/6 = 0.183$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_linear_df = pm_df.interpolate(method=\"linear\")\r\n",
    "pm_linear_df[\"wind_speed\"][31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Nearest\n",
    "\n",
    "The `nearest` strategy is very similar to that of `.fillna()` in that it fills the values. However, this method simply fills in the NaNs with the closest non-NaN value to the current NaN  we're looping over."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_nearest_df = pm_df.interpolate(method=\"nearest\")\r\n",
    "pm_nearest_df[\"wind_speed\"][31230:31245]"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Quadratic\n",
    "Quadratic interpolation attempts to fit a quadratic/parabolic curve between the two non-NaN values. Later in this notebook we will show plots of the different interpolation methods and you'll be able to get a better sense of understanding how and where each of the interpolation methods we are presenting would be useful. We won't be diving into the methodology behind quadratic interpolation, but you can find a brilliant walkthrough here: https://www.youtube.com/watch?v=ifS8LL3qT2g\n",
    "\n",
    "![](../images/quadratic_interpolation.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_quad_df = pm_df.interpolate(method=\"quadratic\")\r\n",
    "pm_quad_df[\"wind_speed\"][31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualising Time-series data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_df[\"wind_speed\"][31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.options.plotting.backend = 'plotly'\r\n",
    "\r\n",
    "fig = make_subplots(rows=6, cols=1, shared_xaxes=False, subplot_titles=(\"None\", \"Ffill Interpoloation\", \"Bfill Interpoloation\", \"Linear Interpoloation\", \"Nearest Interpoloation\", \"Quadratic Interpoloation\"))\r\n",
    "\r\n",
    "default = pm_df[\"wind_speed\"][31230:31245]\r\n",
    "\r\n",
    "fig_none = default.plot()\r\n",
    "fig.add_trace(fig_none[\"data\"][0], row=1, col=1)\r\n",
    "\r\n",
    "figs_list = [pm_ffill_df, pm_bfill_df, pm_linear_df, pm_nearest_df, pm_quad_df]\r\n",
    "for i, to_fig in enumerate(figs_list):\r\n",
    "    fig_ = to_fig[\"wind_speed\"][31230:31245].plot(color_discrete_sequence=[\"red\"])\r\n",
    "    fig_.add_trace(px.line(pm_df[\"wind_speed\"][31230:31245]).data[0])\r\n",
    "    \r\n",
    "    fig.add_trace(fig_[\"data\"][0], row=i+2, col=1)\r\n",
    "    fig.add_trace(fig_[\"data\"][1], row=i+2, col=1)\r\n",
    "\r\n",
    "fig.update_layout(title=\"Wind Speed Interpolations\", width=np.inf, height=1600, showlegend=False)\r\n",
    "fig.update_traces(mode='markers+lines')\r\n",
    "fig.show()\r\n",
    "\r\n",
    "# fig = pm_df.interpolate(method=\"quadratic\")[\"wind_speed\"][31230:31245].plot()\r\n",
    "# fig.add_trace(px.line(pm_df[\"wind_speed\"][31230:31245], color_discrete_sequence=[\"red\"]).data[0])\r\n",
    "# fig.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputing Categorical Variables\n",
    "A naive approach to impute categorical data is to either use the most frequent/mode method or the `.fillna()` strategy that we looked at earlier. However, it is possible to impute categorical variables as we would a typical continuous variable. We cannot directly perform the imputation as straightforwardly as before because the categorical variables are usually encoded as strings (and therefore we can not perform any mathematical operations on them). To impute these kinds of variables, we must first encode them them as numeric values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "knn_impute = KNNImputer(n_neighbors=3, weights=\"distance\")\r\n",
    "\r\n",
    "# We will drop city and country because these are categorical variables which have all their values present\r\n",
    "# We will keep season even though it has all the values present (just to demonstrate the process over multiple categorical variables)\r\n",
    "# We will drop humidity and precipitation_cum as these features are fully NaN\r\n",
    "pm_to_impute_cols = [\"season\", \"pm25\", \"dew_point\", \"temperature\", \"pressure\", \"wind_direction\", \"wind_speed\", \"precipitation_hourly\"]\r\n",
    "pm_to_impute_df = pm_df[pm_to_impute_cols]\r\n",
    "cat_cols = [\"season\", \"wind_direction\"]\r\n",
    "\r\n",
    "# key: col, value: list of tuples\r\n",
    "category_dict_encode = {}\r\n",
    "category_dict_decode = {}\r\n",
    "for col in cat_cols:\r\n",
    "    pm_to_impute_df[col] = pm_to_impute_df[col].astype(\"category\")\r\n",
    "    categories = pm_to_impute_df[col].cat.categories\r\n",
    "    \r\n",
    "    col_cat_dict = dict(enumerate(categories))\r\n",
    "    print(\"Column, Category dict:\", col_cat_dict)\r\n",
    "    category_dict_decode[col] = col_cat_dict\r\n",
    "    \r\n",
    "    col_cat_dict = {v:k for k,v in col_cat_dict.items()}\r\n",
    "    print(\"Inverted Column, Category dict:\", col_cat_dict)\r\n",
    "    category_dict_encode[col] = col_cat_dict\r\n",
    "\r\n",
    "print()\r\n",
    "print(\"Category dict ENCODE:\", category_dict_encode)\r\n",
    "print(\"Category dict DECODE:\", category_dict_decode)\r\n",
    "pm_to_impute_df.replace(category_dict_encode, inplace=True)\r\n",
    "pm_to_impute_df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "knn_pm_arr = knn_impute.fit_transform(pm_to_impute_df)\r\n",
    "knn_pm_df = pd.DataFrame(knn_pm_arr, columns = pm_to_impute_cols)\r\n",
    "knn_pm_df[31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for col in cat_cols:\r\n",
    "    knn_pm_df[col] = knn_pm_df[col].round()\r\n",
    "knn_pm_df.index = pm_df.index\r\n",
    "knn_pm_df[31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In the original dataframe we will only replace the values for the categorical cols we imputed\r\n",
    "# We need to map the columns back to their codes\r\n",
    "knn_pm_df.replace(category_dict_decode, inplace=True)\r\n",
    "knn_pm_df[31230:31245]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "imputed_cols_df = knn_pm_df[cat_cols]\r\n",
    "imputed_cols_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pm_imputed_df = pm_df.copy(deep=True)\r\n",
    "pm_imputed_df.update(knn_pm_df, overwrite=False)\r\n",
    "pm_imputed_df[31230:31245]"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nulls = pm_df[\"wind_speed\"][31230:31245].isnull()\r\n",
    "print(nulls)\r\n",
    "\r\n",
    "fig = pm_imputed_df[\"wind_speed\"][31230:31245].plot(color_discrete_sequence=[\"red\"])\r\n",
    "fig.add_trace(px.line(pm_df[\"wind_speed\"][31230:31245]).data[0])\r\n",
    "fig.update_traces(mode='markers+lines')\r\n",
    "fig.update_layout(title=\"Wind Speed Interpolation (KNN Full DF imputation)\")\r\n",
    "fig[\"data\"][0][\"name\"] = \"Interpolated\"\r\n",
    "fig[\"data\"][1][\"name\"] = \"Orignal\"\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "ad8bebc098a042dc0df4e42fc2ecc8fff0bd7b8741641ce29007c29766dadbe0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}